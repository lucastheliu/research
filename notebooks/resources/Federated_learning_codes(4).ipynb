{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Attacks in Federated Learning (2)\n",
        "\n",
        "Please complete the following tasks with the help of Claude and ChatGPT:\n",
        "\n",
        "1. **Adapt the CNN code for MNIST** (from code **(3)**) to a federated learning framework.  \n",
        "   - Integrate the CNN model into a setup where multiple clients train locally and send their parameters to a central server.\n",
        "\n",
        "2. **Introduce attacks under i.i.d. data** across communication rounds, following these steps:\n",
        "   - **Single communication round**:  \n",
        "     When the server receives parameters from the clients (before averaging), apply the attack code from code **(3)** to recover each clientâ€™s data.\n",
        "   - **Multiple communication rounds**:  \n",
        "     Fix the attack to target only *client 1*. In each round, once the server receives parameters from client 1, execute the attack once. After \\( T \\) rounds, the server will have \\( T \\) separate attack results. **Compare** these results to evaluate how the attack evolves over multiple rounds.\n"
      ],
      "metadata": {
        "id": "2H_IMHu-xuwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import OrderedDict"
      ],
      "metadata": {
        "id": "VkWZnPG2i_9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "JRbW74yEjBv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "\n",
        "        self.dropout1 = nn.Dropout2d(0.25)\n",
        "        self.dropout2 = nn.Dropout2d(0.5)\n",
        "\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        x = F.softmax(x, dim=1)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "_N1hNDnPjDiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_mnist():\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "    test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
        "\n",
        "    return train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "aOYAPoZrjMYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def distribute_data(dataset, num_clients, iid=True):\n",
        "    if iid:\n",
        "        num_items_per_client = len(dataset) // num_clients\n",
        "        client_datasets = []\n",
        "\n",
        "        indices = torch.randperm(len(dataset))\n",
        "        for i in range(num_clients):\n",
        "            start_idx = i * num_items_per_client\n",
        "            end_idx = (i + 1) * num_items_per_client if i < num_clients - 1 else len(dataset)\n",
        "            client_indices = indices[start_idx:end_idx]\n",
        "            client_datasets.append(Subset(dataset, client_indices))\n",
        "    else:\n",
        "        labels = dataset.targets.numpy()\n",
        "        sorted_indices = np.argsort(labels)\n",
        "        client_datasets = []\n",
        "        shards_per_client = 2\n",
        "\n",
        "        num_shards = num_clients * shards_per_client\n",
        "        items_per_shard = len(dataset) // num_shards\n",
        "        shard_indices = []\n",
        "\n",
        "        for i in range(num_shards):\n",
        "            start_idx = i * items_per_shard\n",
        "            end_idx = (i + 1) * items_per_shard if i < num_shards - 1 else len(sorted_indices)\n",
        "            shard_indices.append(sorted_indices[start_idx:end_idx])\n",
        "\n",
        "        np.random.shuffle(shard_indices)\n",
        "\n",
        "        for i in range(num_clients):\n",
        "            client_idx = []\n",
        "            for j in range(shards_per_client):\n",
        "                client_idx.extend(shard_indices[i * shards_per_client + j])\n",
        "            client_datasets.append(Subset(dataset, client_idx))\n",
        "\n",
        "    return client_datasets"
      ],
      "metadata": {
        "id": "gL-3oMtpjNyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Client:\n",
        "    def __init__(self, dataset, client_id, device):\n",
        "        self.dataset = dataset\n",
        "        self.client_id = client_id\n",
        "        self.device = device\n",
        "        self.model = CNN().to(device)\n",
        "        self.dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    def train(self, epochs=1):\n",
        "        self.model.train()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            for batch_idx, (data, target) in enumerate(self.dataloader):\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                output = self.model(data)\n",
        "                loss = F.cross_entropy(output, target)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "        return total_loss / len(self.dataloader)\n",
        "\n",
        "    def evaluate(self, test_loader):\n",
        "        self.model.eval()\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, target in test_loader:\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                output = self.model(data)\n",
        "                test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "                pred = output.argmax(dim=1, keepdim=True)\n",
        "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        accuracy = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "        return test_loss, accuracy\n",
        "\n",
        "    def get_parameters(self):\n",
        "        return {k: v.cpu() for k, v in self.model.state_dict().items()}\n",
        "\n",
        "    def set_parameters(self, parameters):\n",
        "        params_on_device = {k: v.to(self.device) for k, v in parameters.items()}\n",
        "        self.model.load_state_dict(params_on_device)"
      ],
      "metadata": {
        "id": "KVqSXyfzjQoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Server:\n",
        "    def __init__(self, test_dataset, device):\n",
        "        self.clients = []\n",
        "        self.device = device\n",
        "        self.global_model = CNN().to(device)\n",
        "        self.test_loader = DataLoader(test_dataset, batch_size=128)\n",
        "\n",
        "    def add_client(self, client):\n",
        "        self.clients.append(client)\n",
        "\n",
        "    def aggregate_parameters(self, client_parameters):\n",
        "        global_dict = OrderedDict()\n",
        "\n",
        "        for k in client_parameters[0].keys():\n",
        "            global_dict[k] = torch.stack([client_parameters[i][k] for i in range(len(client_parameters))], 0).mean(0)\n",
        "\n",
        "        return global_dict\n",
        "\n",
        "    def update_global_model(self):\n",
        "        client_parameters = [client.get_parameters() for client in self.clients]\n",
        "        global_parameters = self.aggregate_parameters(client_parameters)\n",
        "\n",
        "        self.global_model.load_state_dict({k: v.to(self.device) for k, v in global_parameters.items()})\n",
        "\n",
        "        for client in self.clients:\n",
        "            client.set_parameters(global_parameters)\n",
        "\n",
        "        return client_parameters[0]  # Return client 1's parameters for potential attack\n",
        "\n",
        "    def evaluate_global_model(self):\n",
        "        self.global_model.eval()\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, target in self.test_loader:\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                output = self.global_model(data)\n",
        "                test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "                pred = output.argmax(dim=1, keepdim=True)\n",
        "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "        test_loss /= len(self.test_loader.dataset)\n",
        "        accuracy = 100. * correct / len(self.test_loader.dataset)\n",
        "\n",
        "        return test_loss, accuracy"
      ],
      "metadata": {
        "id": "PuOBJ1FzjUcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelInversionAttack:\n",
        "    def __init__(self, model_params, target_class, device):\n",
        "        self.model_params = model_params\n",
        "        self.target_class = target_class\n",
        "        self.device = device\n",
        "\n",
        "    def perform_multimodel_attack(self, models_list, num_iterations=1000, learning_rate=0.01, reg_param=0.01):\n",
        "        \"\"\"\n",
        "        Perform model inversion attack using multiple models with different parameters.\n",
        "\n",
        "        Args:\n",
        "            models_list: List of model parameters for different models\n",
        "            num_iterations: Number of optimization iterations\n",
        "            learning_rate: Learning rate for optimization\n",
        "            reg_param: Regularization parameter for total variation loss\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (recovered image, loss history)\n",
        "        \"\"\"\n",
        "        loaded_models = []\n",
        "        for model_params in models_list:\n",
        "            model = CNN().to(self.device)\n",
        "            model.load_state_dict({k: v.to(self.device) for k, v in model_params.items()})\n",
        "            model.eval()\n",
        "            loaded_models.append(model)\n",
        "\n",
        "        # Initialize the recovered image\n",
        "        recovered_image = torch.rand(1, 1, 28, 28, requires_grad=True, device=self.device)\n",
        "        optimizer = optim.Adam([recovered_image], lr=learning_rate)\n",
        "\n",
        "        # One-hot encode target class\n",
        "        target = torch.zeros(1, 10, device=self.device)\n",
        "        target[0, self.target_class] = 1\n",
        "\n",
        "        losses = []\n",
        "\n",
        "        for i in range(num_iterations):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Calculate classification loss across all models\n",
        "            classification_loss = 0\n",
        "            for model in loaded_models:\n",
        "                pred = model(recovered_image)\n",
        "                classification_loss += -torch.sum(target * torch.log(pred + 1e-10))\n",
        "\n",
        "            # Average the classification loss\n",
        "            classification_loss /= len(loaded_models)\n",
        "\n",
        "            # Total variation regularization\n",
        "            tv_loss = torch.sum(torch.abs(recovered_image[:, :, :, :-1] - recovered_image[:, :, :, 1:])) + \\\n",
        "                      torch.sum(torch.abs(recovered_image[:, :, :-1, :] - recovered_image[:, :, 1:, :]))\n",
        "            tv_loss = tv_loss * reg_param\n",
        "\n",
        "            # L2 regularization\n",
        "            l2_loss = torch.sum(recovered_image ** 2) * 0.001\n",
        "\n",
        "            # Combine all losses\n",
        "            total_loss = classification_loss + tv_loss + l2_loss\n",
        "\n",
        "            # Backpropagate and update\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Clamp values to valid image range\n",
        "            with torch.no_grad():\n",
        "                recovered_image.clamp_(0, 1)\n",
        "\n",
        "            # Log progress\n",
        "            if i % 400 == 0:\n",
        "                losses.append(classification_loss.item())\n",
        "                print(f\"[{i}, {classification_loss.item():.4f}]\")\n",
        "\n",
        "        # Convert final result to numpy array\n",
        "        with torch.no_grad():\n",
        "            recovered_image_np = recovered_image.cpu().numpy()\n",
        "\n",
        "        return recovered_image_np, losses\n",
        "\n",
        "    def perform_attack(self, num_iterations=1000, learning_rate=0.01, reg_param=0.01):\n",
        "        model = CNN().to(self.device)\n",
        "        model.load_state_dict({k: v.to(self.device) for k, v in self.model_params.items()})\n",
        "        model.eval()\n",
        "\n",
        "        recovered_image = torch.rand(1, 1, 28, 28, requires_grad=True, device=self.device)\n",
        "        # recovered_image_2 = torch.rand(1, 1, 28, 28, requires_grad=True, device=self.device)\n",
        "\n",
        "        # optimizer = optim.Adam([recovered_image, recovered_image_2], lr=learning_rate)\n",
        "        optimizer = optim.Adam([recovered_image], lr=learning_rate)\n",
        "\n",
        "        target = torch.zeros(1, 10, device=self.device)\n",
        "        target[0, self.target_class] = 1\n",
        "\n",
        "        losses = []\n",
        "\n",
        "        for i in range(num_iterations):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            pred = model(recovered_image)\n",
        "            # pred_2 = model(recovered_image_2)\n",
        "\n",
        "            classification_loss = -torch.sum(target * torch.log(pred + 1e-10)) #-torch.sum(target * torch.log(pred_2 + 1e-10))\n",
        "\n",
        "            tv_loss = torch.sum(torch.abs(recovered_image[:, :, :, :-1] - recovered_image[:, :, :, 1:])) + \\\n",
        "                    torch.sum(torch.abs(recovered_image[:, :, :-1, :] - recovered_image[:, :, 1:, :]))\n",
        "            tv_loss = tv_loss * reg_param\n",
        "\n",
        "            # tv_loss_2 = torch.sum(torch.abs(recovered_image_2[:, :, :, :-1] - recovered_image_2[:, :, :, 1:])) + \\\n",
        "                    # torch.sum(torch.abs(recovered_image_2[:, :, :-1, :] - recovered_image_2[:, :, 1:, :]))\n",
        "            # tv_loss_2 = tv_loss_2 * reg_param\n",
        "\n",
        "            l2_loss = torch.sum(recovered_image ** 2) * 0.001\n",
        "            # l2_loss_2 = torch.sum(recovered_image_2 ** 2) * 0.001\n",
        "\n",
        "            total_loss = classification_loss + tv_loss + l2_loss #+ tv_loss_2 + l2_loss_2\n",
        "\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                recovered_image.clamp_(0, 1)\n",
        "                # recovered_image_2.clamp_(0, 1)\n",
        "\n",
        "            if i % 800 == 0:\n",
        "                losses.append(classification_loss.item())\n",
        "                print(f\"[{i}, {classification_loss.item():.4f}]\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            recovered_image_np = recovered_image.cpu().numpy()\n",
        "            # recovered_image_np = ((recovered_image + recovered_image_2) / 2).cpu().numpy()\n",
        "\n",
        "        return recovered_image_np, losses"
      ],
      "metadata": {
        "id": "HcbwFxNJjYIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_federated_learning(num_clients=3, num_rounds=3, local_epochs=1, iid=False):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    train_dataset, test_dataset = load_mnist()\n",
        "    client_datasets = distribute_data(train_dataset, num_clients, iid=iid)\n",
        "\n",
        "    server = Server(test_dataset, device)\n",
        "    clients = []\n",
        "    client_params_list = []\n",
        "\n",
        "    for i in range(num_clients):\n",
        "        client = Client(client_datasets[i], i, device)\n",
        "        clients.append(client)\n",
        "        server.add_client(client)\n",
        "\n",
        "    global_accuracies = []\n",
        "    client_losses = [[] for _ in range(num_clients)]\n",
        "\n",
        "    # Federated learning loop\n",
        "    for round_num in range(num_rounds):\n",
        "        client_params_list.append([])\n",
        "\n",
        "        print(f\"\\nRound {round_num+1}/{num_rounds}\")\n",
        "\n",
        "        for i, client in enumerate(clients):\n",
        "            loss = client.train(epochs=local_epochs)\n",
        "            client_losses[i].append(loss)\n",
        "            print(f\"Client {i+1} loss: {loss:.4f}\")\n",
        "\n",
        "            client_params_list[round_num].append(client.get_parameters())\n",
        "\n",
        "        server.update_global_model()\n",
        "        test_loss, accuracy = server.evaluate_global_model()\n",
        "        global_accuracies.append(accuracy)\n",
        "        print(f\"Global model - Test loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    return (global_accuracies, client_losses), (server, clients, client_params_list)"
      ],
      "metadata": {
        "id": "zEWVJIm4janG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_clients = 10\n",
        "\n",
        "(global_accuracies, client_losses), (server, clients, client_params_list) = run_federated_learning(\n",
        "    num_clients=num_clients,\n",
        "    num_rounds=10,\n",
        "    local_epochs=1,\n",
        "    iid=True\n",
        ")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_params = [clients[i].get_parameters() for i in range(num_clients)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLyD1RW0cjR6",
        "outputId": "fbb83544-40ee-4eb1-f050-a15ceec76b5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Round 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:1538: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client 1 loss: 1.7766\n",
            "Client 2 loss: 1.8427\n",
            "Client 3 loss: 1.8103\n",
            "Client 4 loss: 1.8062\n",
            "Client 5 loss: 1.7920\n",
            "Client 6 loss: 1.7787\n",
            "Client 7 loss: 1.8516\n",
            "Client 8 loss: 1.7749\n",
            "Client 9 loss: 1.8039\n",
            "Client 10 loss: 1.8605\n",
            "Global model - Test loss: 2.3020, Accuracy: 11.35%\n",
            "\n",
            "Round 2/10\n",
            "Client 1 loss: 1.8507\n",
            "Client 2 loss: 1.8370\n",
            "Client 3 loss: 1.8455\n",
            "Client 4 loss: 1.8636\n",
            "Client 5 loss: 1.8677\n",
            "Client 6 loss: 1.8600\n",
            "Client 7 loss: 1.8467\n",
            "Client 8 loss: 1.8514\n",
            "Client 9 loss: 1.8642\n",
            "Client 10 loss: 1.8940\n",
            "Global model - Test loss: 1.6395, Accuracy: 83.17%\n",
            "\n",
            "Round 3/10\n",
            "Client 1 loss: 1.7044\n",
            "Client 2 loss: 1.6887\n",
            "Client 3 loss: 1.6859\n",
            "Client 4 loss: 1.6880\n",
            "Client 5 loss: 1.6945\n",
            "Client 6 loss: 1.6937\n",
            "Client 7 loss: 1.6814\n",
            "Client 8 loss: 1.6871\n",
            "Client 9 loss: 1.6777\n",
            "Client 10 loss: 1.6845\n",
            "Global model - Test loss: 1.5671, Accuracy: 89.58%\n",
            "\n",
            "Round 4/10\n",
            "Client 1 loss: 1.6607\n",
            "Client 2 loss: 1.6549\n",
            "Client 3 loss: 1.6565\n",
            "Client 4 loss: 1.6503\n",
            "Client 5 loss: 1.6521\n",
            "Client 6 loss: 1.6554\n",
            "Client 7 loss: 1.6425\n",
            "Client 8 loss: 1.6521\n",
            "Client 9 loss: 1.6484\n",
            "Client 10 loss: 1.6469\n",
            "Global model - Test loss: 1.5521, Accuracy: 90.94%\n",
            "\n",
            "Round 5/10\n",
            "Client 1 loss: 1.6337\n",
            "Client 2 loss: 1.6292\n",
            "Client 3 loss: 1.6190\n",
            "Client 4 loss: 1.6267\n",
            "Client 5 loss: 1.6315\n",
            "Client 6 loss: 1.6226\n",
            "Client 7 loss: 1.6189\n",
            "Client 8 loss: 1.6270\n",
            "Client 9 loss: 1.6286\n",
            "Client 10 loss: 1.6300\n",
            "Global model - Test loss: 1.5384, Accuracy: 92.43%\n",
            "\n",
            "Round 6/10\n",
            "Client 1 loss: 1.6173\n",
            "Client 2 loss: 1.6032\n",
            "Client 3 loss: 1.6029\n",
            "Client 4 loss: 1.5959\n",
            "Client 5 loss: 1.5983\n",
            "Client 6 loss: 1.6039\n",
            "Client 7 loss: 1.5987\n",
            "Client 8 loss: 1.5944\n",
            "Client 9 loss: 1.6024\n",
            "Client 10 loss: 1.5997\n",
            "Global model - Test loss: 1.5281, Accuracy: 93.43%\n",
            "\n",
            "Round 7/10\n",
            "Client 1 loss: 1.5862\n",
            "Client 2 loss: 1.5824\n",
            "Client 3 loss: 1.5795\n",
            "Client 4 loss: 1.5797\n",
            "Client 5 loss: 1.5753\n",
            "Client 6 loss: 1.5824\n",
            "Client 7 loss: 1.5720\n",
            "Client 8 loss: 1.5803\n",
            "Client 9 loss: 1.5749\n",
            "Client 10 loss: 1.5729\n",
            "Global model - Test loss: 1.5171, Accuracy: 94.55%\n",
            "\n",
            "Round 8/10\n",
            "Client 1 loss: 1.5711\n",
            "Client 2 loss: 1.5637\n",
            "Client 3 loss: 1.5663\n",
            "Client 4 loss: 1.5579\n",
            "Client 5 loss: 1.5580\n",
            "Client 6 loss: 1.5673\n",
            "Client 7 loss: 1.5558\n",
            "Client 8 loss: 1.5564\n",
            "Client 9 loss: 1.5658\n",
            "Client 10 loss: 1.5583\n",
            "Global model - Test loss: 1.5093, Accuracy: 95.19%\n",
            "\n",
            "Round 9/10\n",
            "Client 1 loss: 1.5561\n",
            "Client 2 loss: 1.5509\n",
            "Client 3 loss: 1.5543\n",
            "Client 4 loss: 1.5436\n",
            "Client 5 loss: 1.5472\n",
            "Client 6 loss: 1.5583\n",
            "Client 7 loss: 1.5460\n",
            "Client 8 loss: 1.5465\n",
            "Client 9 loss: 1.5518\n",
            "Client 10 loss: 1.5446\n",
            "Global model - Test loss: 1.5038, Accuracy: 95.75%\n",
            "\n",
            "Round 10/10\n",
            "Client 1 loss: 1.5405\n",
            "Client 2 loss: 1.5435\n",
            "Client 3 loss: 1.5410\n",
            "Client 4 loss: 1.5331\n",
            "Client 5 loss: 1.5315\n",
            "Client 6 loss: 1.5432\n",
            "Client 7 loss: 1.5364\n",
            "Client 8 loss: 1.5377\n",
            "Client 9 loss: 1.5395\n",
            "Client 10 loss: 1.5396\n",
            "Global model - Test loss: 1.4984, Accuracy: 96.34%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(client_params_list[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aavLAk0PTbnj",
        "outputId": "adcd3a20-7574-4ca8-879e-f5d4d72b34f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "target_classes = range(0, 9 + 1)\n",
        "\n",
        "for target_class in target_classes:\n",
        "    print(f\"Finished class: {target_class}\")\n",
        "\n",
        "    images = []\n",
        "\n",
        "    attack = ModelInversionAttack(None, target_class, device)\n",
        "    test_loader = torch.utils.data.DataLoader(datasets.MNIST('./data', train=False, transform=transforms.ToTensor()), batch_size=64)\n",
        "\n",
        "    attacked_image, _ = attack.perform_multimodel_attack(\n",
        "        # [\n",
        "        #     client_params_list[0][0], # round 1, client 0\n",
        "        #     client_params_list[1][0], # round 2, client 0\n",
        "        #     client_params_list[2][0], # round 3, client 0\n",
        "        #     client_params_list[3][0], # round 4, client 0\n",
        "        #     client_params_list[4][0]  # round 5, client 0\n",
        "        # ],\n",
        "        model_params,\n",
        "        num_iterations=400, # 1000\n",
        "        learning_rate=0.1,  # 0.01\n",
        "        reg_param=1e-4     # 1e-5\n",
        "    )\n",
        "    attacked_image = np.squeeze(attacked_image)\n",
        "\n",
        "    images.append(attacked_image)\n",
        "    results.append(images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-Gt6TZnRn6-",
        "outputId": "81ea9826-a82d-4c17-ed41-ff2ac55dd732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished class: 0\n",
            "[0, 2.5600]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:1538: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished class: 1\n",
            "[0, 2.5712]\n",
            "Finished class: 2\n",
            "[0, 2.2056]\n",
            "Finished class: 3\n",
            "[0, 1.1223]\n",
            "Finished class: 4\n",
            "[0, 2.7920]\n",
            "Finished class: 5\n",
            "[0, 2.2885]\n",
            "Finished class: 6\n",
            "[0, 3.7979]\n",
            "Finished class: 7\n",
            "[0, 3.0912]\n",
            "Finished class: 8\n",
            "[0, 2.4416]\n",
            "Finished class: 9\n",
            "[0, 3.4616]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "target_classes = range(10)\n",
        "\n",
        "for i in range(num_clients):\n",
        "    print(f\"Attacking client: {i}\")\n",
        "    images = []\n",
        "\n",
        "    for target_class in target_classes:\n",
        "        attack = ModelInversionAttack(model_params[i], target_class, device)\n",
        "        test_loader = torch.utils.data.DataLoader(datasets.MNIST('./data', train=False, transform=transforms.ToTensor()), batch_size=64)\n",
        "\n",
        "        attacked_image, _ = attack.perform_multimodel_attack(model_params, num_iterations=1000, learning_rate=0.01, reg_param=1e-5)\n",
        "        attacked_image = np.squeeze(attacked_image)\n",
        "        images.append(attacked_image)\n",
        "\n",
        "        print(f\"Finished class: {target_class}\")\n",
        "\n",
        "    results.append(images)"
      ],
      "metadata": {
        "id": "7_Ze05O2dNOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fig, axes = plt.subplots(len(target_classes), num_clients, figsize=(10, 10))\n",
        "fig, axes = plt.subplots(1, 2, figsize=(8, 8))\n",
        "\n",
        "# for row in range(len(target_classes)):\n",
        "for row in range(2):\n",
        "    # for col in range(num_clients):\n",
        "    for col in range(1):\n",
        "        axes[row][col].imshow(results[col][row], cmap='gray')\n",
        "\n",
        "        if row == 0:\n",
        "            axes[row][col].set_title(f\"Client: {col + 1}\")\n",
        "\n",
        "        axes[row][col].get_xaxis().set_visible(False)\n",
        "\n",
        "        if col == 0:\n",
        "            axes[row][col].set_ylabel(f\"Class: {target_classes[row]}\")\n",
        "        else:\n",
        "            axes[row][col].get_yaxis().set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KSArjylAlLyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCYj1_-TVLEV",
        "outputId": "ce35c0ee-88fc-4213-b7c7-b2a39d88148e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(3, 4, figsize=(10, 6))\n",
        "\n",
        "round = 0\n",
        "\n",
        "for y in range(3):\n",
        "    for x in range(4):\n",
        "        if round <= 9:\n",
        "            axes[y][x].imshow(results[round][0], cmap='gray')\n",
        "        axes[y][x].axis(\"off\")\n",
        "        round += 1\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "bCeaI4_FU6IK",
        "outputId": "690f6d11-6136-4d9e-d2d0-279bab1fd29d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 12 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qYashM_8U5hY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NcF-6QEDREg3"
      }
    }
  ]
}