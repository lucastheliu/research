{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b07935e9",
   "metadata": {},
   "source": [
    "# Inversion Attack On Models\n",
    "\n",
    "This notebook requires both the `mlp_model.pth` file from `mlp.ipynb` and the `clients/` model files from  `federated.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36c00ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('../src'))\n",
    "\n",
    "from models import MLP, CNN\n",
    "from utils import get_mnist_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d4533be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InversionAttack:\n",
    "    def __init__(self, loaded_models, target_class, device):\n",
    "        self.loaded_models = loaded_models\n",
    "        self.target_class = target_class\n",
    "        self.device = device\n",
    "\n",
    "    def perform_multimodel_attack(self, num_iter=1000, lr=0.01, reg_param=0.01):\n",
    "        # Initialize the recovered image\n",
    "        recovered_image = torch.rand(1, 1, 28, 28, requires_grad=True, device=self.device)\n",
    "        optimizer = optim.Adam([recovered_image], lr=lr)\n",
    "\n",
    "        # One-hot encode target class\n",
    "        target = torch.zeros(1, 10, device=self.device)\n",
    "        target[0, self.target_class] = 1\n",
    "\n",
    "        losses = []\n",
    "\n",
    "        for i in range(num_iter):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            classification_loss = 0\n",
    "            for model in self.loaded_models:\n",
    "                pred = model(recovered_image)\n",
    "                classification_loss += -torch.sum(target * torch.log(pred + 1e-10))\n",
    "\n",
    "            # Average the classification loss\n",
    "            classification_loss /= len(self.loaded_models)\n",
    "\n",
    "            # Total variation regularization\n",
    "            tv_loss = torch.sum(torch.abs(recovered_image[:, :, :, :-1] - recovered_image[:, :, :, 1:])) + \\\n",
    "                      torch.sum(torch.abs(recovered_image[:, :, :-1, :] - recovered_image[:, :, 1:, :]))\n",
    "            tv_loss = tv_loss * reg_param\n",
    "\n",
    "            # L2 regularization\n",
    "            l2_loss = torch.sum(recovered_image ** 2) * 0.001\n",
    "\n",
    "            # Combine all losses\n",
    "            total_loss = classification_loss + tv_loss + l2_loss\n",
    "\n",
    "            # Backpropagate and update\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Clamp values to valid image range\n",
    "            with torch.no_grad():\n",
    "                recovered_image.clamp_(0, 1)\n",
    "\n",
    "            # Log progress\n",
    "            if i % 400 == 0:\n",
    "                losses.append(classification_loss.item())\n",
    "                print(f\"[{i}, {classification_loss.item():.4f}]\")\n",
    "\n",
    "        # Convert final result to numpy array\n",
    "        with torch.no_grad():\n",
    "            recovered_image_np = recovered_image.cpu().numpy()\n",
    "\n",
    "        options = {\n",
    "            'losses': losses,\n",
    "            'target_class': self.target_class,\n",
    "            'device': self.device,\n",
    "            'num_iter': num_iter,\n",
    "            'lr': lr,\n",
    "            'reg_param': reg_param\n",
    "        }\n",
    "\n",
    "        return recovered_image_np, options\n",
    "\n",
    "    def perform_attack(self, epochs=1000, lr=0.01, reg_param=0.01):\n",
    "        model = CNN().to(self.device)\n",
    "        model.load_state_dict({k: v.to(self.device) for k, v in self.model_params.items()})\n",
    "        model.eval()\n",
    "\n",
    "        recovered_image = torch.rand(1, 1, 28, 28, requires_grad=True, device=self.device)\n",
    "\n",
    "        optimizer = optim.Adam([recovered_image], lr=lr)\n",
    "\n",
    "        target = torch.zeros(1, 10, device=self.device)\n",
    "        target[0, self.target_class] = 1\n",
    "\n",
    "        losses = []\n",
    "\n",
    "        for i in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pred = model(recovered_image)\n",
    "\n",
    "            classification_loss = -torch.sum(target * torch.log(pred + 1e-10))\n",
    "\n",
    "            tv_loss = torch.sum(torch.abs(recovered_image[:, :, :, :-1] - recovered_image[:, :, :, 1:])) + \\\n",
    "                    torch.sum(torch.abs(recovered_image[:, :, :-1, :] - recovered_image[:, :, 1:, :]))\n",
    "            tv_loss = tv_loss * reg_param\n",
    "\n",
    "            l2_loss = torch.sum(recovered_image ** 2) * 0.001\n",
    "\n",
    "            total_loss = classification_loss + tv_loss + l2_loss\n",
    "\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                recovered_image.clamp_(0, 1)\n",
    "\n",
    "            if i % 800 == 0:\n",
    "                losses.append(classification_loss.item())\n",
    "                print(f\"[{i}, {classification_loss.item():.4f}]\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            recovered_image_np = recovered_image.cpu().numpy()\n",
    "\n",
    "        return recovered_image_np, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513e3101",
   "metadata": {},
   "source": [
    "## Attack and Analysis\n",
    "\n",
    "The following code block contains the creation and execution of the inversion attack class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f6ba266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded models!\n"
     ]
    }
   ],
   "source": [
    "def load_models(device='cpu'):\n",
    "    if not os.path.exists('../models/mlp_model.pth') or not os.path.exists('../models/clients/model-0.pth'):\n",
    "        print('Missing required model files.')\n",
    "        return None, None\n",
    "\n",
    "    mlp_model = MLP()\n",
    "    mlp_model.load_state_dict(torch.load('../models/mlp_model.pth'))\n",
    "    mlp_model.to(device)\n",
    "    mlp_model.eval()\n",
    "\n",
    "    cnn_models = []\n",
    "    index = 0\n",
    "\n",
    "    while True:\n",
    "        model_path = os.path.join('../models/clients', f'model-{index}.pth')\n",
    "        if not os.path.exists(model_path):\n",
    "            break\n",
    "\n",
    "        model = CNN()\n",
    "        state_dict = torch.load(model_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        cnn_models.append(model)\n",
    "        index += 1\n",
    "\n",
    "    return mlp_model, cnn_models\n",
    "\n",
    "device = ('mps' if torch.mps.is_available() else 'cpu')\n",
    "mlp_model, cnn_models = load_models(device)\n",
    "\n",
    "print('Successfully loaded models!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27435ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucastheliu/Projects/research/.venv/lib/python3.13/site-packages/torch/nn/functional.py:1538: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2.3767]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader, test_loader = get_mnist_loader()\n",
    "\n",
    "attack = InversionAttack(\n",
    "    loaded_models=cnn_models,\n",
    "    target_class=0,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "result, options = attack.perform_multimodel_attack(\n",
    "    num_iter=400,\n",
    "    lr=0.1,\n",
    "    reg_param=1e-4\n",
    ")\n",
    "\n",
    "image = np.squeeze(result)\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title(f'Reconstructed Image ({options['target_class']})')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
